{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<div> <center> ESPACIO PARA BANNER DE LA MAESTRIA </center> </div>   \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XX - Latent Dirichlet Allocation (LDA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La asignación latente de Dirichlet (LDA) es una técnica de clustering que puede ser aplicada a colecciones de datos discretos como los son los documentos de texto. LDA es un modelo bayesiano jerárquico de tres niveles en donde cada elemento o palabra de un texto se modela como una mezcla finita de tópicos. A su vez, cada tópico se modela como una combinación infinita de palabras. En este contexto, decimos que LDA es un modelo probabilístico generativo pues estas distribuciones resultan en una representación explícita de cada conjunto de datos o documento.\n",
    "\n",
    "Esta técnica de aprendizaje no supervisado se diferencia de las técnicas de clustering estudiadas anteriormente en el curso porque en este caso cada observación pertenece a más de un grupo. En ese orden de ideas, se puede decir que LDA es una técnica de agrupamiento difuso (*fuzzy o soft clustering*) donde la pertenencia de un elemento a un grupo se modela como una distribución de probabilidades.\n",
    "\n",
    "De hecho, la palabra *Dirichlet* dentro del nombre de la técnica hace alusión al Proceso Dirichlet, desarrollado por el matemático alemán Peter Gustav Dirichlet, la cual consiste en una familia de procesos estocásticos en las cuales sus realizaciones son distribuciones de probabilidad. Por otro lado, la palabra *Latante* se refiere a que los tópicos de los que tratan los textos están ocultos o por lo menos no están explícitos.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notación\n",
    "A continuación se hará uso de entidades como \"palabras\", \"documentos\" y \"corpus\" con el fin de guiar la intuición. Sin embargo, eso no quiere decir que el uso de LDA sea exclusivo en textos. Otras posibles aplicaciones de LDA son el filtrado colaborativo, la recuperación de imagenes basadas en contenido y la bioinformática.\n",
    "\n",
    "Formalmente, se definen los siguientes términos:\n",
    "- Una *palabra* es la unidad básica de los datos discretos. Definimos una palabra como un item de un vocabulario indexado por $\\{1, \\cdots, V\\}$. Se representan las palabras mediante vectores de base uno, es decir, un vector donde solo un elemento es 1 y el resto son 0. Así, usando superíndices para denotar componentes, la v-ésima palabra en el vocabulario se representa mediante un V-vector $w$ tal que $w^v = 1$ y $w^u = 0$ para $u\\neq v$.\n",
    "- Un *documento* es una secuencia de $N$ palabras denotadas por $\\mathbf{w}=(w_1,w_2,\\cdots,w_N)$, en donde $w_n$ es la eneava palabra de la secuencia. \n",
    "- Un *corpus* es una colección de $M$ documentos dentodas por $\\mathbf{D}=(\\mathbf{w_1},\\mathbf{w_2},\\cdots,\\mathbf{w_m})$.\n",
    "\n",
    "Este modelo tiene algunos supuestos como:\n",
    "- Cada documento es solo una colección de palabras o una \"bolsa de palabras\". Así, el orden de las palabras y el rol gramatical de las palabras (sujeto, predicado, verbos, ...) no se consideran en el modelo.\n",
    "- Palabras como a/el/la/pero/y/o/... no contienen ninguna información sobre los \"temas\" y, por lo tanto, pueden eliminarse de los documentos como un paso de preprocesamiento. De hecho, podemos eliminar palabras que aparecen en al menos %80 ~ %90 de los documentos, sin perder ninguna información. Por ejemplo, si nuestro corpus contiene solo documentos médicos, palabras como humano, cuerpo, salud, etc. pueden estar presentes en la mayoría de los documentos y, por lo tanto, pueden eliminarse ya que no agregan ninguna información específica que haga que el documento se diferencie del resto. \n",
    "- Sabemos de antemano cuántos temas queremos. $k$ está predeterminado.\n",
    "- Los documentos son una combinación de temas.\n",
    "- Los temas son una combinación de palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up del modelo\n",
    "LDA es un modelo probabilístico generativo del *corpus*. La idea general es que los documentos se representan como una combinación aleatoria de los tópicos latentes, donde cada tópico está caracterizado por una distribución sobre las palabras.\n",
    "LDA supone los siguientes procesos generadores de cada documento $\\mathbf(w)$ en el corpus $D$:\n",
    "1. Escoger $N\\sim Poisson(\\xi)$.\n",
    "2. Escoger $\\theta\\sim Dir(\\alpha)$.\n",
    "3. Para cada palabra $w_n$ de las $N$ palabras:\n",
    "\n",
    "    a. Escoger un tópico $z_n\\sim Multinomial(\\theta)$.\n",
    "    \n",
    "    b. Escoger una palabra $w_n$ de $p(w_n|z_n, \\beta)$, una probabilidad multinomial condicionada en el tópico $z_n$.\n",
    "\n",
    "Este modelo inicial cuenta con algunas simplificaciones que más adelante serán removidas. En primer lugar, la dimensionalidad $k$ de la distribución Dirichlet (y por consiguiente la dimensionalidad de la variables de los tópicos $z$) se supone fija y conocida. Segundo, las probabilidades de cada palabra son parametrizadas por una matriz $\\beta$ de tamaño $k\\times V$ en donde $\\beta_{ij}=p(w^j = 1|z^i = 1)$, que por ahora trataremos como una cantidad fija que será estimada. Finalmente, el supuesto de $N$ distribuido Poisson no es critico ya que otras distribuciones más realistas sobre la longitud de los documentos pueden ser usadas según la necesidad del investigador. Más aún, note que $N$ es independiente del resto de variables generadoras ($\\theta$ y $\\mathbf{z}$) por tanto ignoraremos su aleatoriedad en el desarrollo posterior.\n",
    "\n",
    "Una variable aleatoria $\\theta$ de dimensión $k$ puede tomar valores en el $(k-1)-símplex$ (un vector de dimensión $k$ llamado $\\theta$ cae en el $(k-1)-símplex$ si $\\theta_i\\geq0, \\sum_{i=1}^k\\theta_i=1$), y tiene la siguiente función de densidad en el símplex:\n",
    "\n",
    "$$p(\\theta|\\alpha)=\\frac{\\Gamma(\\sum_{i=1}^k\\alpha_i)}{\\prod_{i=1}^k\\Gamma(\\alpha_i)}\\theta_1^{(\\alpha_1-1)}\\cdots \\theta_k^{(\\alpha_k-1)}$$\n",
    "\n",
    "En donde el parámetro $\\alpha$ es un vector de dimensión $k$ con componentes $a_i$ tal que $a_i\\geq0$, y $\\Gamma(x)$ es la función Gamma. La Dirichlet es una distribución conveniente para el símplex porque está en la familia exponencial, tiene estadísticas suficientes de dimensión finita y es conjugada a la distribución multinomial.\n",
    "\n",
    "Antes de continuar, vale la pena profundizar un poco sobre qué es un símplex y por qué es relevante en este modelo. Podemos pensar en el LDA como una aproximación geométrica en donde el símplex es una figura con $k$ vertices equidistantes entre ellos. Un 0-símplex es un punto, un 1-símplex es un segmento de línea, un 2-símplex es un triángulo, un 3-símplex es un tetraedro y un 4-símplex es un pentácoron.\n",
    "\n",
    "<center>\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e1/Simplexes.jpg/800px-Simplexes.jpg\" alt = \"simplex\" style = \"width: 500px;\"/>\n",
    "</center>\n",
    "\n",
    "En el caso de LDA, los documentos dentro del corpus se distribuirían a lo largo del símplex en donde cada vértice se representa un tópico. Luego, cada documento se ubicaría más cercano a los vértices que representan los tópicos contenidos en él. Por ejemplo, supongamos que tenemos 7 documentos y tres tópicos posibles (Deporte, Ciencia y Política), podríamos representar los documentos dentro del símplex de la siguiente manera:\n",
    "\n",
    "<center>\n",
    "<img src = \"data/2-simplex.png\" alt = \"LDA\" style = \"width: 500px;\"/>\n",
    "</center>\n",
    "\n",
    "De esta manera podríamos ver que cada documento es una combinación de tópicos. El documento 1 sería 100% sobre Deportes, el documento 2 sería 50% sobre Deportes y 50% sobre Ciencia, el documento 3 sería 100% sobre Ciencia, etc.\n",
    "\n",
    "Continuando con nuestra explicación del modelo. Tomando como dados los parámetros $\\alpha$ y $\\beta$, la distribución de probabilidad conjunta de una mezcla de tópicos $\\theta$, un conjunto de $N$ temas $\\mathbf{z}$ y un conjunto de $N$ palabras $\\mathbf{w}$ es:\n",
    "\n",
    "$$p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta) = p(\\theta|\\alpha)\\prod_{n=1}^N p(z_n|\\theta)p(w_n|z_n, \\beta)$$\n",
    "\n",
    "En donde $p(z_n|\\theta)$ es simplemente $\\theta_i$ para el único $i$ en donde $z_n^i=1$. \n",
    "\n",
    "A continuación se muestra una representación gráfica del modelo. El rectángulo exterior M representa los documentos y el rectángulo interior N representa la escogencia repetida de palabras ($w$) y tópicos ($z$) al interior de un documento.\n",
    "\n",
    "<center>\n",
    "<img src = \"data/LDA1.png\" alt = \"LDA1\" style = \"width: 500px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la expresión anterior podemos decir que el lado izquierdo de la ecuación ($p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)$) corresponde a la probabilidad de que un documento $x$ aparezca en nuestro corpus. De este modo, nuestro objetivo será estimar $\\alpha$ y $\\beta$ de modo que se maximice la probabilidad de encontrar nuestra muestra de documentos.\n",
    "\n",
    "La Figura anterior corresponde a una ilustración de la ecuación en cuestión. En donde $\\alpha$ es una distribución Dirichlet que le asigna una combinación de tópicos a cada uno de los documentos. $\\beta$ es una distribución Dirichlet que le asigna una combinación de palabras a cada uno de los tópicos. A partir de $\\alpha$ obtenemos $\\theta$ la cual es una distribución multinomial de donde se extraen $N$ tópicos. Luego, para cada uno de los tópicos se extrae una palabra de la distribución multinomial que sale de $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia variacional\n",
    "El principal problema de inferencia al que nos enfrentamos a la hora de usar LDA es el cálculo de la distribución posterior de las variables escondidas dado un documento:\n",
    "\n",
    "$$p(\\theta, \\mathbf{z}|\\mathbf{w},\\alpha, \\beta)=\\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}|\\alpha, \\beta)}{p(\\mathbf{w}|\\alpha, \\beta)}$$\n",
    "\n",
    "Desafortunadamente, esta distribución no se puede calcular directamente, por ende se debe aproximar numéricamente. Entre los métodos que existen para aproximarnos a la solución están la aproximación de Laplace, Aproximación variacional, Cadena de Makov Monte Carlo, etc.\n",
    "\n",
    "En esta sección vamos a describir la intuición detrás de un algoritmo variacional simple basado en la convexidad. La idea principal detrás de este método es hacer uso de la [Desigualdad de Jensen](https://es.wikipedia.org/wiki/Desigualdad_de_Jensen) para obtener un límite inferior ajustable en la log-verosimilitud. En esencia, se considera una familia de límites inferiores indexada a un conjunto de *parámetros variacionales*. Los parámetros variacionales se eligen mediante un procedimiento de optimización que intenta encontrar el límite inferior más ajustado posible.\n",
    "\n",
    "De forma más intuitiva podemos explicar el algoritmo de la siguiente manera:\n",
    "1. Comience asignando aleatoriamente cada palabra de cada documento del corpus a uno de los temas.\n",
    "2. Para cada documento y cada palabra en cada documento por separado, calcule dos proporciones:\n",
    "    - La proporción de palabras en el documento que están actualmente asignadas al tema: $p(Tema|Documento)$\n",
    "    - La proporción de asignaciones en todos los documentos de una palabra específica al tema: $p(Palabra|Tema)$\n",
    "3. Multiplique las dos proporciones y use la proporción resultante para asignar la palabra a un nuevo tema. \n",
    "4. Repita este proceso hasta que se alcance un estado estable en el que las asignaciones de temas no cambien significativamente. Estas asignaciones luego se utilizan para estimar la mezcla de temas dentro del documento y la mezcla de palabras dentro del tema.\n",
    "\n",
    "La lógica detrás de la inferencia variacional es que, si la distribución real es intratable, entonces se debe encontrar una distribución más simple, llamémosla distribución variacional, muy cercana a la distribución verdadera, que es manejable, para que la inferencia sea posible. En otras palabras, dado que es imposible inferir la distribución real debido a la complejidad de la distribución real, tratamos de encontrar una distribución más simple que sea una excelente aproximación de la distribución real.\n",
    "\n",
    "En nuestro caso, el problema de inferencia se da por el acomplamiento entre los parámetros $\\beta$ y $\\theta$ debido a los enlaces entre $\\theta$, $\\mathbf{z}$ y $\\mathbf{w}$. Eliminando estas aristas y los nodos $\\mathbf{w}$, y dotando al modelo gráfico simplificado resultante de parámetros variacionales libres, obtenemos una familia de distribuciones de las variables latentes. Esta familia se caracteriza por la siguiente distribución variacional:\n",
    "\n",
    "$$q(\\theta, \\mathbf{z} | \\gamma, \\phi) = q(\\theta|\\gamma)\\prod_{n=1}^N q(z_n|\\phi_n)$$\n",
    "\n",
    "En donde $\\gamma$ es el parámetro Dirichlet y los parámetros multinomiales $(\\phi_1,\\cdots, \\phi_N)$ son los parámetros variacionales libres. A continuación se ilustra gráficamente la representación del modelo de la distribución variacional utilizada para aproximar la distribución posterior en LDA.\n",
    "\n",
    "<center>\n",
    "<img src = \"data/LDA2.png\" alt = \"LDA2\" style = \"width: 300px;\"/>\n",
    "</center>\n",
    "\n",
    "Para comenzar, se selecciona una familia de distribuciones (es decir, binomial, gaussiana, exponencial, etc.), $q$, condicionada a los nuevos parámetros variacionales. Los parámetros están optimizados para que la distribución original (que en realidad es la distribución posterior), y la distribución variacional sean lo más parecidas posible. La distribución variacional será lo suficientemente cercana a la distribución posterior original para ser utilizada como proxy, haciendo que cualquier inferencia hecha sobre ella sea aplicable a la distribución posterior original.\n",
    "\n",
    "El objetivo de encontrar un límite inferior ajustado en la log-verosimilitud se traduce directamente en el siguiente problema de optimización para determinar los parámetros variacionales $\\gamma$ y $\\phi$:\n",
    "\n",
    "$$(\\gamma^*,\\phi^*)=\\argmin_{(\\gamma,\\phi)} D(q(\\theta, \\mathbf{z}|\\gamma,\\phi)\\ ||\\ p(\\theta, \\mathbf{z}|\\mathbf{w}, \\alpha, \\beta))$$\n",
    "\n",
    "Hay una gran colección de distribuciones variacionales potenciales que se pueden usar como una aproximación para la distribución posterior. Se selecciona una distribución variacional inicial de la colección, que actúa como punto de partida para un proceso de optimización que iterativamente se acerca más y más a la distribución óptima. Los parámetros óptimos son los parámetros de la distribución que mejor se aproximan al posterior. La similitud de las dos distribuciones se mide utilizando la divergencia de [Kullback-Leibler (KL)](https://es.wikipedia.org/wiki/Divergencia_de_Kullback-Leibler). La divergencia KL representa la cantidad esperada de error generado si aproximamos una distribución con otra. La distribución con parámetros óptimos tendrá la divergencia KL más pequeña cuando se mida con la distribución real. \n",
    "\n",
    "Una vez que se ha identificado la distribución óptima, lo que significa que se han identificado los parámetros óptimos, se puede aprovechar para producir las matrices de salida y ejecutar cualquier inferencia requerida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "- Blei, D. M., Jordan, M. I., &; Ng, A. Y. (2003). Latent Dirichlet Allocation. JMLR.org, 3, 993–1022. https://doi.org/10.5555/944919.944937 "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe36d3cf18f454bb22b210d1ce52ae8c21a1b2f0a9257a143474ae90bef14b60"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
