{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<div> <center> ESPACIO PARA BANNER DE LA MAESTRIA </center> </div>   \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento del Lenguaje Natural\n",
    "\n",
    "Este *cuaderno* trata sobre el análisis de canasta de compra. Este análisis ayuda a los vendedores a comprender las preferencias de sus clientes y mejorar sus sistemas de recomendación. El objetivo del *cuaderno* es que usted aprenda que es el análisis de canasta de compra, los conceptos teóricos y el algoritmo que lo implementa, reconociendo sus  características y funcionamiento, y cómo ejecutarlo.\n",
    "\n",
    "\n",
    "**NO** es necesario editar el archivo o hacer una entrega. Sin embargo, los ejemplos contienen celdas con código ejecutable (`en gris`), que podrá modificar  libremente. Esta puede ser una buena forma de aprender nuevas funcionalidades del *cuaderno*, o experimentar variaciones en los códigos de ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Usuario: Me siento triste.*\n",
    "\n",
    "*ELIZA: ¿Cree que venir acá puede ayudarle a no sentirse triste?*\n",
    "\n",
    "*Usuario: Necesito ayuda, de eso estoy seguro.*\n",
    "\n",
    "*ELIZA: ¿Qué significa para usted obtener ayudar?*\n",
    "\n",
    "*Usuario: A lo mejor podría aprender a llevármela mejor con mi madre.*\n",
    "\n",
    "*ELIZA: Cuénteme más sobre su madre.*\n",
    " \n",
    "*Usuario: Mi madre cuida de mi.*\n",
    "\n",
    "*ELIZA: ¿Quién más en su familia cuida de usted?*\n",
    "\n",
    "*Usuario: Mi padre.*\n",
    "\n",
    "*ELIZA: ¿Su padre?*\n",
    "\n",
    "*Usuario: De alguna manera usted se parece a mi padre.*\n",
    "\n",
    "Weizenbaum (1966)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La conversación anterior fue realizada en 1966 por un chatbot llamado ELIZA y un ser humano. ELIZA fue uno de los primeros sistemas de procesamiento del lenguaje natural el cual estaba programado para imitar las respuestas de un psicoterapeuta Rogersiano. En el fondo, el programa de ELIZA es simple pues este está entrenado para identificar frases del estilo \"Necesito X\" y responder frases del estilo \"¿Qué significa para usted obtener X?\".\n",
    "\n",
    "El éxito de ELIZA consiste en que este no tiene que tener conocimientos previos sobre el significado de las palabras porque este tipo de diálogos permiten que el que hace de terapeuta no tenga que saber nada sobre el mundo. De hecho, Weizenbaum (1966) reporta que muchos de las personas que interactuaron con este bot creyeron que ELIZA en verdad los estaba entendiendo, inclusive, luego de que se les explicara a los participantes cómo funcionaba ELIZA, muchos seguían pensando que este robot era util para ayudarlos a lidiar con sus problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy en día, los algoritmos de Procesamiento de Lenguaje Natural son mucho más complejos que ELIZA, sin embargo, la detección de patrones en el texto sigue siendo una base fundamental de este tipo de aplicaciones. En particular, se le conoce como **Expresiones Regulares** a la secuencia de caracteres que especifican la búsqueda de un patrón dentro de un texto. Como veremos a continuación, esto nos servirá de herramienta para extraer información relevante pero también para hacer todo el pre procesamiento necesario para limpiar el texto antes de introducirlo en algún algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización del texto\n",
    "La limpieza del texto para ponerlo en el formato estándar que se necesite de acuerdo a nuestro objetivo final se le conoce como *normalización*. A grandes rasgos, podemos hablar de 3 procesos importantes de normalización:\n",
    "1. **Tokenización.** Consiste en separar el texto en palabras. En primera instancia parece fácil, pues la mayoría de palabras se separan con un espacio en blanco, sin embargo, algunos conceptos están hechos de varias palabras. Por ejemplo, si tenemos un texto que tiene las palabras Nueva York y Economía Naranja, lo correcto sería tomar como una sola palabra \"Nueva York\" y \"Economía Naranja\" porque si se parten por los espacios se cambiaría el significado del texto. Adicionalmente, uno de los grandes retos que se tiene es tratar con textos producidos por humanos que no son revisados por un tercero, por ejemplo los posts de redes sociales. En estos deja de ser cierto que las palabras siempre se separen con un espacio. Los humanos están sujetos a cometer errores por lo que frecuentemente se les olvida poner un espacio o incluyen más de un espacio para separar las palabras. \n",
    "\n",
    "    La correcta Tokenización también depende del contexto del escrito. Por ejemplo, en inglés, la palabra \"I'm\" se debe separar en \"I\" y \"am\" y en Japones las palabras no se separan con espacio, luego se deben tener en cuenta otras consideraciones para tokenizar idiomas diferentes al español. Además, los emojis o los hashtags deben ser tenidos en cuenta en este proceso.\n",
    "\n",
    "2. **Lematización.** Para algunas aproximaciones, es importante tratar de unificar las palabras que se refieren al mismo concepto. Por ejemplo, si se quiere hacer un conteo simple para crear una nube de palabras de un documento, quisiéramos que corriendo, corrió y correrá no se traten como palabras independientes. Lematización consiste en devolver las palabras a una misma raíz. Por ende, corriendo, corrió y correrá se transformarían en el verbo en infinitivo *correr*. \n",
    "\n",
    "    Una versión simplificada de la lematización es el **stemming** el cual consiste en remover el sufijo o el final de las palabras. Por ejemplo, corrieron y correr se volverían *corr*. \n",
    "\n",
    "3. **Distancia entre palabras.** Para comparar palabras y conceptos, quisiéramos tener una noción de qué tan diferentes son unas de otras. Una de las métricas más sencillas se llama *la distancia de edición* que consiste en contar el número de adiciones, eliminaciones y sustituciones que hay que hacerle a una palabra $i$ para que se convierta en la palabra $j$. Una aplicación de esto se encuentra en los algoritmos de corrección de ortografía y detección de discurso. Otras métricas más complejas serán estudiadas en el siguiente modulo donde estudiaremos la distancia de las palabras en un espacio vectorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, antes de adentrarnos en el cómo normalizar los textos, comenzaremos con las nociones básicas de las expresiones regulares o RegEx. Una de las ventajas de esta herramienta es que es transversal a los diferentes lenguajes de programación, incluso se pueden hacer búsquedas en la consola o command prompt de su computador utilizando la función `grep`.\n",
    "\n",
    "## Patrones básicos\n",
    "El patrón más básico que se puede utilizar con expresiones regulares es una secuencia de caracteres a buscar en cuestión. Por ejemplo, si quisiéramos buscar la palabra *tienda* en un texto, simplemente podríamos usar como patrón `tienda`. Los patrones de búsqueda pueden estar conformados por un solo carácter como `!` para buscar signos de exclamación o también una secuencia de letras `urgl`.\n",
    "\n",
    "Este tipo de búsquedas es sensible al uso de mayúsculas, por ejemplo, buscar la palabra `tienda` arroja un resultado diferente al de buscar `Tienda`. Del mismo modo, también es sensible al uso de caracteres especiales como tildes, apostrofes, entre otros. Eliminar estos caracteres especiales para simplificar el texto es bastante recomendado. Por ejemplo, transformar un texto como *A palabras necias oídos sordos* por *a palabras necias oidos sordos* hará más sencillo su tratamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> <center> \n",
    "\n",
    "| **RE** |      **Ejemplo del patrón capturado**     |\n",
    "|:------:|:-----------------------------------------:|\n",
    "| tienda | El que tenga <u>tienda</u> que la atienda |\n",
    "|    a   |    El que m<u>a</u>druga Dios le ayuda    |\n",
    "|    !   |            !Ojo con eso<u>!</u>           |\n",
    "\n",
    "</center> </div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obstante, las expresiones regulares nos permiten hacer uso de los corchetes cuadrados (`[]`) para expresar una disyunción lógica sobre los caracteres dentro de los corchetes. Por ejemplo, la búsqueda `[Tt]ienda` sirve para encontrar la palabra `tienda` o la palabra `Tienda`. Los corchetes indican que se busca una palabra que contenga la cadena `ienda` precedida por una letra `t` en minúscula *o* mayúscula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> <center> \n",
    "\n",
    "|    **RE**    |**Patrón capturado**|          **Ejemplo del patrón capturado**            |\n",
    "|:------------:|:----------------:|:------------------------------------------------------:|\n",
    "|   [Tt]ienda  |  Tienda o tienda |        El que tiene <u>tienda</u> que la atienda       |\n",
    "|     [abc]    |   a, b **o** c   | No me <u>a</u>bra los ojos que no le voy a echar gotas |\n",
    "| [1234567890] | Cualquier dígito |           Eramos al rededor de <u>5</u> a 8 personas   |\n",
    "\n",
    "</div> </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vea que la expresión regular `[1234567890]` le permite capturar cualquier dígito, no obstante, escribir bloques de dígitos o letras puede ser inconveniente. Para capturar cualquier letra no es muy práctico escribir `[abcdefghijklmnopqrstuvwxyz]`. Por eso, se puede completar una búsqueda dentro de corchetes con un guión (`-`) de este modo se especifica un rango. Por ejemplo `[0-9]` nos permite capturar cualquier número entre 0 y 9, `[b-g]` nos permite capturar cualquier letra de la `b` a la `g` o sea *b, c, d, e, f **o** g*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> <center>\n",
    "\n",
    "| **RE** |     **Patrón capturado**     |            **Ejemplo del patrón capturado**            |\n",
    "|:------:|:----------------------------:|:------------------------------------------------------:|\n",
    "|  [A-Z] | Cualquier letra en mayúscula |        <u>E</u>l que tiene tienda que la atienda       |\n",
    "|  [a-z] | Cualquier letra en minúscula | N<u>o</u> me abra los ojos que no le voy a echar gotas |\n",
    "|  [0-9] |       Cualquier dígito       |       Eramos al rededor de <u>5</u> a 8 personas       |\n",
    "\n",
    "</div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los paréntesis cuadrados también sirven para indicar que caracteres no deben ser capturados usando un caret (`^`). Si y solo si el caret (`^`) es el primer símbolo dentro de los paréntesis cuadrados, el patrón subsiguiente es negado. Por ejemplo `[^a]` significa que se va a capturar cualquier caracter, incluyendo los especiales, excepto la letra *a*. Si se usa el caret (`^`) en cualquier otro lugar de la expresión regular, este no va a significar una negación sino un caret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> <center>\n",
    "\n",
    "| **RE** |               **Patrón capturado**              |            **Ejemplo del patrón capturado**            |\n",
    "|:------:|:-----------------------------------------------:|:------------------------------------------------------:|\n",
    "| [^A-Z] | Cualquier caracter menos una letra en mayúscula |       E<u>l</u> que tiene tienda que la atienda       |\n",
    "|  [^Ss] |     Cualquier caracter excepto una \"s\" o \"S\"    | <u>N</u>o me abra los ojos que no le voy a echar gotas |\n",
    "|  [^.]  |        Cualquier caracter menos un punto        |       <u>E</u>ramos al rededor de 5 a 8 personas       |\n",
    "|  [e^]  |             Captura una \"e\" o un \"^\"            |                       <u>e</u>^x                       |\n",
    "|  [a^b] |                   Captura a^b                   |                 La expresión <u>a^b</u>                |\n",
    "\n",
    "</div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del mismo modo, a veces queremos capturar patrones de forma opcional. Por ejemplo, para capturar una palabra en plural o en singular en donde el último caracter puede ser una *s*. Para hacer esto utilizamos el símbolo de pregunta (`?`) después del caracter que queremos dejar como opcional. El signo de pregunta (`?`) en el contexto de expresiones regulares significa el carácter anterior o ninguno.\n",
    "\n",
    "Ahora, en el lenguaje también tenemos casos donde un caracter se puede repetir más de una vez. Por ejemplo, en un libro se podría encontrar el sonido de una vaca de las siguientes formas:\n",
    "\n",
    "Muu!\n",
    "\n",
    "Muuu!\n",
    "\n",
    "Muuuu!\n",
    "\n",
    "Muuuuu!\n",
    "\n",
    "A grandes rasgos se tiene que el sonido de una vaca comienza con una *M* y luego con por lo menos dos *u* y termina con el signo de exclamación *!*. La expresión regular que nos permite capturar cero o más ocurrencias de un caracter es el asterisco (`\\*`) también conocido como *cleany star* o *Kleene \\**. Por ende, la expresión regular `u*` va a capturar tanto `u` como `uuuuuu`, pero a su vez también podría capturar `vaca` pues esta palabra tiene cero letras u. Para corregir esto podríamos usar la expresión regular `uu*` la cual significa una o más letras u. Algunos patrones más complejos también se pueden utilizar haciendo uso de los corchetes, por ejemplo `[ab]*` sirve para capturar cero o más as o bs. Por ende, se capturarían textos como *aaaaa*, *bbb* o *ababababab*.\n",
    "\n",
    "Para especificar múltiples dígitos podemos usar `[0-9][0-9]*` para capturar cualquier entero. Sin embargo, aún podemos simplificar nuestras expresiones regulares más. Podemos usar el signo de suma o más (`+`), también llamado *Kleene +*, para denotar que el caracter a capturar se repite una vez o más. Por ende, la expresión `[0-9]+` es la forma más común de expresar una secuencia de dígitos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> <center>\n",
    "\n",
    "|  **RE**  | **Patrón capturado** |            **Ejemplo del patrón capturado**           |\n",
    "|:--------:|:--------------------:|:-----------------------------------------------------:|\n",
    "| tiendas? | \"tienda\" o \"tiendas\" |       El que tenga <u>tienda</u> que la atienda       |\n",
    "|  colou?r |  \"color\" o \"colour\"  | Discover the newest hand-picked <u>color</u> palettes |\n",
    "|   mu+!   | mu! con una o más us |              La vaca hizo <u>muuuuu!</u>              |\n",
    "|   baa*   |  ba con una o más as |               La cabra hace <u>baaa</u>!              |\n",
    "|  [0-9]+  |   Cualquier entero   |              Ese camisa cuesta $<u>25</u>             |\n",
    "\n",
    "</div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro de los caracteres más importantes es el punto (`.`) pues este funciona como comodín o *wildcard*. Esta expresión regular sirve para capturar cualquier caracter excepto los saltos de línea. \n",
    "\n",
    "También tenemos algo denominado anclas o *anchors* que sirven para capturar elementos en posiciones particulares del texto. Los más comunes son el caret (`^`) y el símbolo de dolar (`$`) los cuales hacen alusión al inicio y final de un texto respectivamente. Por ejemplo, la expresión `^El` solo captura la palabra *El* si y solo si está al inicio del corpus de texto. Otras anclas comunes son (`\\b`) y (`\\B`) que denotan los *boundaries* o limites de una palabra o dentro de una palabra respectivamente. Por ejemplo, `\\bel\\b` va a capturar la palabra *el* pero no *elefante*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disyunción, agrupación y precedencia\n",
    "En algunos casos estamos interesados en buscar más de una palabra a la vez. Por ejemplo, si quisiéramos buscar países de Latinoamérica en un texto, no podríamos escribir `[ColombiaPerúChileMéxico...]` pues esto solo nos devolvería alguna de las letras presentes en los corchetes. El operador de disyunción (`|`) sirve para este tipo de casos donde estamos interesados en una u otra palabra, por eso, el patrón `Colombia|Perú|Chile` devuelve Colombia, Perú o Chile. \n",
    "\n",
    "A su vez, puede que nos interesemos no solo en los países como tal, sino también en las personas de los países. Por eso, si quisiéramos extraer Chile o Chilenos necesitamos sofisticar un poco nuestro operador de disyunción para evitar escribir la expresión `Chile|Chilenos`. En este caso podemos hacer uso de los paréntesis para explicitar que la disyunción solo aplica para una parte del texto: `Chil(e|enos)`. Note que si omitiéramos los paréntesis `Chile|enos` solo se capturaría *Chile* o *enos* y dado que *Chile* tiene precedencia sobre *enos* en caso de encontrar la palabra *Chilenos* solo se haría el match hasta *Chile* sin el *nos*.\n",
    "\n",
    "Los paréntesis también son un gran complemento para los asteriscos (`*`). Suponga que se tiene el índice de un libro que tiene el siguiente texto: Capítulo 1 Capítulo 2 Capítulo 3, etc. Para capturar todos los capítulos quisieramos crear un patrón que capturara repetidamente la palabra *Capítulo* seguida de algún entero. La expresión regular `Capítulo [0-9]+ *` se queda corta pues solo captura *Capítulo* seguida de un entero y 0 más espacios. Para corregir esto, hacemos uso de los paréntesis: `(Capítulo [0-9]+ *)*`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunos operadores adicionales\n",
    "\n",
    "<div> <center>\n",
    "\n",
    "| **RE** | **Expansión** |                          **Patrón capturado**                         |\n",
    "|:------:|:-------------:|:---------------------------------------------------------------------:|\n",
    "|   \\d   |     [0-9]     |                            Cualquier dígito                           |\n",
    "|   \\D   |     [^0-9]    |                          Cualquier no dígito                          |\n",
    "|   \\w   |  [a-zA-Z0-9_] |                  Cualquier alfanumérico o guion bajo                  |\n",
    "|   \\W   |     [ˆ\\w]     |                 Cualquier no alfanumérico o guion bajo                |\n",
    "|   \\s   |  [ \\r\\t\\n\\f]  |                           Espacio en blanco                           |\n",
    "|   \\S   |     [ˆ\\s]     |                        No un espacio en blanco                        |\n",
    "|    *   |               |         Cero o más ocurrencias del caracter o expresión pasada        |\n",
    "|    +   |               |         Una o más ocurrencias del caracter o expresión pasada         |\n",
    "|    ?   |               | Exactamente cero o una ocurrencia del del caracter o expresión pasada |\n",
    "|   {n}  |               |            *n* ocurrencias del caracter o expresión pasada            |\n",
    "|  {n,m} |               |        De *n* a *m* ocurrencias del caracter o expresión pasada       |\n",
    "|  {n,}  |               |      Por lo menos *n* ocurrencias del caracter o expresión pasada     |\n",
    "|  {,m}  |               |         Hasta *m* ocurrencias del caracter o expresión pasada         |\n",
    "\n",
    "</div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es usual usar alguna página como https://regexr.com para probar el correcto funcionamiento de las expresiones regulares creadas antes de utilizarlas en el código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "- Jurafsky, D., &; Martin, J. H. (2020). Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition. Pearson.\n",
    "- Weizenbaum, J. 1966. ELIZA – A computer program for the study of natural language communication between man and machine. CACM, 9(1):36–45.\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
