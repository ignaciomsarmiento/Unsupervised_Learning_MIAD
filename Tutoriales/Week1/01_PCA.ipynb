{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de Componentes Principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajar directamente con datos de alta dimensión, como ser texto e imágenes, conlleva algunas dificultades: es difícil de analizar, la interpretación es difícil, la visualización es casi imposible y (desde un punto de vista práctico) el almacenamiento de los vectores de datos puede ser muy costos. \n",
    "\n",
    "Sin embargo, los datos de alta dimensión a menudo tienen propiedades que podemos aprovechar. Por ejemplo, los datos de alta dimensión suelen estar demasiado completos, es decir, muchas dimensiones son redundantes y pueden explicarse mediante una combinación de otras dimensiones. \n",
    "\n",
    "Además, las dimensiones en los datos de alta dimensión a menudo se correlacionan de modo que los datos posean una estructura intrínseca de dimensiones inferiores. La reducción de dimensionalidad aprovecha la estructura y la correlación y nos permite trabajar con una representación más compacta de los datos, idealmente sin perder información. Podemos pensar en la reducción de dimensionalidad como una técnica de compresión, similar a jpeg o mp3, que son algoritmos de compresión para imágenes y música.\n",
    "En este capítulo, discutiremos el análisis de componentes principales (PCA), un algoritmo para la reducción de dimensionalidad lineal. La PCA, propuesta por Pearson (1901) y Hotelling (1933), existe desde hace más de 100 años y sigue siendo una de las técnicas más utilizadas para la compresión y visualización de datos. También se utiliza para la identificación de patrones simples, factores latentes y estructuras de datos de alta dimensión. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta notebooks vamos a explorar el hallazgo de las componentes principales en alguno de los datasets vistos en clases anteriores. La idea de componentes principales (y de muchas otras técnicas de reducción dimensional) es encontrar una combinación de los features originales que condensen gran parte de la variabilidad de nuestros datos. La utilidad de esto radica en poder:\n",
    "- visualizar los datos en un espacio mucho más chico que el espacio original;\n",
    "- encontrar direcciones que condensen la variación de features fuertemente correlacionados y, por lo tanto, eliminar información redundante;\n",
    "- alimentar modelos de regresión o clasificación con menos variables independientes;\n",
    "- comprimir información (parte 2).\n",
    "\n",
    "La descomposición en componentes principales es parte del conjunto de algoritmos conocidos como de **aprendizaje no-supervisado**. Esto se debe a que estos algoritmos trabajan sobre el conjunto de features, sin que exista una variable que querramos predecir (variable *target*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiendo el Problema\n",
    "\n",
    "\n",
    "  $X = (x_1 , x_2 , \\dots , x_n )_{N \\times K}$\n",
    "\n",
    "\n",
    "Factor: \n",
    "\n",
    "\n",
    "  $F = X\\delta \\,\\,\\,\\delta \\in K$\n",
    "\n",
    "\n",
    "\n",
    "- Idea: summarize the K variables in a single (F).\n",
    "- Vocab: the coefficients of $\\delta$ are the loadings: how much 'matters' each x s in the factor.\n",
    "- Dimensionality: summarize the original K variables in a few $q <K$ factors.\n",
    "\n",
    "\n",
    "\n",
    "### Algebra Review\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Factors via main components}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- $x_1, x_2, \\dots, x_K$ , K vectors of N observations each.\n",
    "\n",
    "- Factor: $F = X\\delta$\n",
    "\n",
    "- What is the 'best' linear combination of $x_1, x_2, \\dots, x_K$ ?\n",
    "\n",
    "- Best? Maximum variance. Why? The one that best reproduces variability original of all xs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Factors via main components}\n",
    "\n",
    "\n",
    "  - Let\n",
    "  \n",
    "    - $X = (x_1 , \\dots , x_K)_{N \\times K}$  , \n",
    "    - $\\Sigma V(X)$ \n",
    "    - $\\delta \\in K$\n",
    " \n",
    "  \n",
    "  - $F = X\\delta$ is a linear combination of $X$, with $V (X\\delta) = \\delta' \\Sigma \\delta$.\n",
    "  \n",
    "  - Let's set up the problem as \n",
    "  \\begin{align}\n",
    "  \\underset{\\delta}{max}\\,\\,\\, \\delta' \\Sigma \\delta\n",
    "  \\end{align}\n",
    " \n",
    "  - It is obvious that the solution is to bring $\\delta$ to infinity. \n",
    " \n",
    "\n",
    " \n",
    "\n",
    "### Factors via main components}\n",
    "\n",
    "\n",
    "- Let's \"fix\" the problem by normalizing $\\delta$\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{\\delta}{max} \\delta' \\Sigma \\delta \\\\ \\nonumber\n",
    "\\text{subject to}  \\\\ \\nonumber\n",
    "\\delta' \\delta = 1 \\nonumber\n",
    "\\end{align}\n",
    "- Let us call the solution to this problem $\\delta^*$. \n",
    "\n",
    "- $F^* = X\\delta^*$ is the 'best' linear combination of X. \n",
    "\n",
    "- Result: $\\delta^*$ is the eigenvector corresponding to the largest eigenvalue of $\\Sigma = V (X)$.\n",
    "\n",
    "- $F^* = X\\delta^*$ is the first principal component of $X$.\n",
    "\n",
    "- Intuition: $X$ has $K$ columns and $Y = X\\delta$ has only one. The factor built with the first principal component is the best way to represent the K variables of X using a single single variable.\n",
    "\n",
    "- Let\n",
    "  \n",
    "    - $X = (x_1 , \\dots , x_K)_{N \\times K}$  , \n",
    "    - $\\Sigma = V(X)$ \n",
    "    - $\\delta \\in K$\n",
    " \n",
    "  \n",
    "  - $F = X\\delta$ is a linear combination of $X$, with $V (X\\delta) = \\delta' \\Sigma \\delta$.\n",
    "  \n",
    "  - Let's set up the problem as \n",
    "  \\begin{align}\n",
    "  \\underset{\\delta}{max}\\,\\,\\, \\delta' \\Sigma \\delta\n",
    "  \\end{align}\n",
    " \n",
    "  - It is obvious that the solution is to bring $\\delta$ to infinity. \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Let's \"fix\" the problem by normalizing $\\delta$\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{\\delta}{max}\\,\\, \\delta' \\Sigma \\delta \\\\ \\nonumber\n",
    "\\text{subject to}  \\\\ \\nonumber\n",
    "\\delta' \\delta = 1 \\nonumber\n",
    "\\end{align}\n",
    "- Let us call the solution to this problem $\\delta^*$. \n",
    "\n",
    "- $F^* = X\\delta^*$ is the 'best' linear combination of X. \n",
    "\n",
    "- Intuition: $X$ has $K$ columns and $Y = X\\delta$ has only one. The factor built with the first principal component is the best way to represent the K variables of X using a single single variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Solution to the problem of the first principal component\n",
    "- Problem: \n",
    "\\begin{align}\n",
    "\\underset{\\delta}{max}\\,\\, \\delta' \\Sigma \\delta \\,\\, \\text{  s.t.}  \\,\\, \\delta' \\delta = 1 \\nonumber\n",
    "\\end{align}\n",
    "- Seting up the Lagrangian $$\\mathcal{L}(\\delta,\\lambda) = \\delta' \\Sigma \\delta + \\lambda(1-\\delta'\\delta)$$\n",
    "\n",
    "- CPO\n",
    "\n",
    "\\begin{align}\n",
    "\\Sigma \\delta = \\lambda \\delta\n",
    "\\end{align}\n",
    "\n",
    "- At the optimum, $\\delta$ is the eigenvector corresponding to the eigenvalue $\\lambda$ of $\\Sigma$. \n",
    "- Premultiplying by $\\delta$ and  remembering that $\\delta'\\delta = 1$:\n",
    "\\begin{align}\n",
    "\\delta \\Sigma \\delta = \\lambda\n",
    "\\end{align}\n",
    "\\footnotesize\n",
    "- In order to maximize $\\delta \\Sigma \\delta $ we must choose $\\lambda$ equal to the maximum eigenvalue of $\\Sigma$ and $\\delta$ is the corresponding eigenvalue.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\frametitle{Factors is unsupervised learning}\n",
    "\n",
    "\n",
    "\n",
    "- Recall that \n",
    "\n",
    "  - In regression we had\n",
    "  \\begin{align}\n",
    "  y =X \\beta +u\n",
    "  \\end{align}\n",
    " - We minimized the MSE\n",
    " \\begin{align}\n",
    " min \\sum (y_i-\\hat{y})^2\n",
    " \\end{align}\n",
    "\n",
    "- Learning is supervised: the difference between $y$ and $\\hat y$ ``guides'' the learning.\n",
    "\n",
    "-  The factor construction problem is unsupervised: we construct an index (the factor) without ever seeing it.\n",
    "\n",
    "- We start with \n",
    "\\begin{align}\n",
    "X_{n\\times k}\n",
    "\\end{align}\n",
    "- We end  with \n",
    "\\begin{align}\n",
    "F^*_{n\\times 1} = X\\delta^*\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\frametitle{q main components}\n",
    "\n",
    "\n",
    "\n",
    "- The first main component? Are there others?\n",
    "\n",
    "- Let's consider the following problem:\n",
    "\\begin{align}\n",
    "\\underset{\\delta_2}{max}\\,\\, \\delta_2' \\Sigma \\delta_2 \\\\ \\nonumber\n",
    "\\text{subject to}  \\\\ \\nonumber\n",
    "\\delta_2' \\delta_2 &= 1 \\\\ \\nonumber\n",
    "and \\\\ \\nonumber\n",
    "Cov(\\delta'_2 X,\\delta^{*'}X) &=0 \\\\ \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "- $F_2^*=X\\delta^*_2$ is the second principal component : the best linear combination which is\n",
    "orthogonal to the best initial linear combination.\n",
    "- Recursively, using this logic you can form q  main components. \n",
    "- Note that algebraically we could construct $q = K$ factors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\frametitle{q main components}\n",
    "\n",
    "\n",
    "- Let $\\lambda_1,\\dots,\\lambda_K$ be the eigenvalues of $\\Sigma = V(X)$, ordered from highest to lowest, and $p_1 , \\dots , p_K$ the corresponding eigenvectors. Let us call $P$ the matrix of eigenvectors.\n",
    "\n",
    "- Result: $\\delta_j = p_j$ , $\\forall j$ ('loadings' of the principal components =ordered eigenvectors of $\\Sigma$).\n",
    "\n",
    "- Let $F_j = X \\delta_j$ , $j = 1, \\dots, K$ be the j-th principal component. It's easy to see that\n",
    "\\begin{align}\n",
    "V (F_j ) = \\delta'_j \\Sigma \\delta_j = p_j P\\Lambda P p_j = \\lambda_j\n",
    "\\end{align}\n",
    "\n",
    "(the variance of the j-th principal component is the j-th ordered eigenvalue of $\\Sigma$).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\frametitle{Relative importance of factors}\n",
    "\n",
    "\n",
    "\n",
    "- The total variance of X is the sum of the variances of $x_j$ , $j = 1, ..., K$, that is $trace(\\Sigma)$\n",
    "- It is easy to show that:\n",
    "\\begin{align}\n",
    "trace(\\Sigma) = trace(P \\Lambda P')= trace(PP' \\Lambda ) = \\sum_{j=1}^K \\lambda_j= \\sum_{j=1}^K V(F_j)\n",
    "\\end{align}\n",
    "- Then\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\lambda_k}{\\sum_{j=1}^K \\lambda_j}\n",
    "\\end{align}\n",
    "\n",
    "- measures the relative importance of the jth principal component.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\frametitle{Selection of factors}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Look at the importance of the first principal components. If the first one explains a lot, there is really only one dimension (one dimension explains almost everything).\n",
    "\\bigskip\n",
    "- The coefficients of the eigenvectors are weights. See how each of the variables 'contributes' in each factor.\n",
    "\\bigskip\n",
    "- Beware of differences in scale. Always standardize \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\frametitle{Selection of factors}\n",
    "\n",
    "\n",
    "- Let the columns of X be standardized, so that each variable has unit variance. \n",
    "- In this case:\n",
    "\n",
    "\\begin{align}\n",
    "trace(\\Sigma) =  \\sum_{j=1}^K V(F_j) = K\n",
    "\\end{align}\n",
    "\n",
    "- and recall $\\sum_{j=1}^K \\lambda_j= \\sum_{j=1}^K V(F_j)$ then\n",
    "\n",
    "\\begin{align}\n",
    " \\sum_{j=1}^K \\lambda_j = K\n",
    "\\end{align}\n",
    "\n",
    "- On average, each factor contributes one unit. When $\\lambda_j>1$, that factor it explains the total variance more than the average. $\\rightarrow$ Retain the factors with $\\lambda_j > 1$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%----------------------------------------------------------------------%\n",
    "## Factor Computation}\n",
    "Useful Tips: Factor Computation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- As a practical aside, note that \\texttt{prcomp} converts x here from sparse to dense matrix storage.\n",
    "\n",
    "- For really big text DTMs, which will be very sparse, this will cause you to run out of memory. \n",
    "\n",
    "- A big data strategy for PCA is to first calculate the covariance matrix for x and then obtain PC rotations as the eigenvalues of this covariance matrix. \n",
    "\n",
    "- The first step can be done using sparse matrix algebra. \n",
    "\n",
    "- The rotations are then available as\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%----------------------------------------------------------------------%\n",
    "## Factor Interpretation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- $F_s = X\\delta_s$ : 'loadings' often suggest that a factor works as a 'index' of a group of variables.\n",
    "\\bigskip\n",
    "- Idea: look at the 'loadings'\n",
    "\\bigskip\n",
    "- Caution: factors via principal components are orthogonal recursively.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Factor Interpretation: Example\n",
    "\n",
    "\n",
    "\n",
    "- {\\bf Congress and \\theme Roll Call Voting}\n",
    "\\bigskip\n",
    "\n",
    "  - Votes in which names and positions are recorded are called `roll calls'.\n",
    "  \n",
    "  - The site {\\tt voteview.com} archives vote records and the R package { \\tt pscl} has tools for this data.\n",
    "  \n",
    "  - 445 members in the last US House  (the $111^{th}$)\n",
    "  \n",
    "  - 1647 votes:  \\theme nea = -1, \\nv yea=+1, \\gr missing = 0.\n",
    "  \n",
    "  - This leads to a large matrix of observations that can probably be reduced to simple factors {\\gr (party)}.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
